<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>从dpg到ddpg | Charley&#39;s Hut</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="ReinforcementLearning" />
  
  
  
  
  <meta name="description" content="肖淇文, Apr&#x2F;20&#x2F;2024  [!important] 本文仅为大纲 I. DPG (Deterministic Policy Gradient)易混淆符号规定：     符号及定义 含义     $r^\gamma_t&#x3D;\sum\limits_{k&#x3D;t}^\infty\gamma^{k-t}r(s_k,a_k)$ discounted reward from time-step $t$">
<meta property="og:type" content="article">
<meta property="og:title" content="从DPG到DDPG">
<meta property="og:url" content="http://charleyhut.github.io/2024/04/21/dpg_ddpg/index.html">
<meta property="og:site_name" content="Charley&#39;s Hut">
<meta property="og:description" content="肖淇文, Apr&#x2F;20&#x2F;2024  [!important] 本文仅为大纲 I. DPG (Deterministic Policy Gradient)易混淆符号规定：     符号及定义 含义     $r^\gamma_t&#x3D;\sum\limits_{k&#x3D;t}^\infty\gamma^{k-t}r(s_k,a_k)$ discounted reward from time-step $t$">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-04-21T09:56:59.299Z">
<meta property="article:modified_time" content="2024-04-21T10:01:45.732Z">
<meta property="article:author" content="Charley Xiao">
<meta property="article:tag" content="ReinforcementLearning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Charley&#39;s Hut" type="application/atom+xml">
  

  

  <link rel="icon" href="/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt; src:url("/css/fonts/FuturaPTBold.otf") format("woff");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt-light; src:url("/css/fonts/FuturaPTBook.otf") format("woff");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt-italic; src:url("/css/fonts/FuturaPTBookOblique.otf") format("woff");font-weight:400;font-style:italic;}
}

  </style>
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>

  
<script src="/js/bootstrap.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >

  
    
<link rel="stylesheet" href="/css/dialog.css">

  

  

  
    <link rel="stylesheet" href="/css/header-post.css" >
  

  
  
  

<meta name="generator" content="Hexo 5.4.2"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/archives">Archives</a> </li>
                
                  <li> <a class="main-nav-link" href="/categories">Categories</a> </li>
                
                  <li> <a class="main-nav-link" href="/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/about">About</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-dpg_ddpg" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      从DPG到DDPG
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2024/04/21/dpg_ddpg/" class="article-date">
	  <time datetime="2024-04-21T09:56:59.299Z" itemprop="datePublished">2024-04-21</time>
	</a>

      
      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>肖淇文, Apr/20/2024</p>
</blockquote>
<p>[!important] 本文仅为大纲</p>
<h2 id="I-DPG-Deterministic-Policy-Gradient"><a href="#I-DPG-Deterministic-Policy-Gradient" class="headerlink" title="I. DPG (Deterministic Policy Gradient)"></a>I. DPG (Deterministic Policy Gradient)</h2><p>易混淆符号规定：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号及定义</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$r^\gamma_t=\sum\limits_{k=t}^\infty\gamma^{k-t}r(s_k,a_k)$</td>
<td style="text-align:center">discounted reward from time-step $t$</td>
</tr>
<tr>
<td style="text-align:center">$J(\pi)=\mathbb{E}[r_1^\gamma\</td>
<td style="text-align:center">\pi]$</td>
<td>performance objective</td>
</tr>
<tr>
<td style="text-align:center">$p(s\to s’,t,\pi)$</td>
<td style="text-align:center">density at state $s’$ after $t$ steps from $s$</td>
</tr>
<tr>
<td style="text-align:center">$\rho^\pi(s’)=\int_{\mathcal S}\sum\limits\limits_{t=1}^\infty\gamma^{t-1}p_1(s)p(s\to s’,t,\pi){\rm d} s$</td>
<td style="text-align:center">discounted state distribution</td>
</tr>
<tr>
<td style="text-align:center">$V^\pi(s)=\mathbb{E}[r_1^\gamma\</td>
<td style="text-align:center">S_1=s;\pi]$</td>
<td>state value</td>
</tr>
<tr>
<td style="text-align:center">$Q^\pi(s,a)=\mathbb{E}[r_1^\gamma\</td>
<td style="text-align:center">S_1=s,A_1=a;\pi]$</td>
<td>action value</td>
</tr>
</tbody>
</table>
</div>
<p>因此有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\pi_\theta)&=\int_{\mathcal S}\rho^\pi(s)\int_{\mathcal A}\pi_\theta(s,a)r(s,a){\rm d}a{\rm d}s\\ 
&=\mathbb{E}_{s\sim\rho^\pi,a\sim\pi_\theta}[r(s,a)]
\end{aligned}</script><h3 id="I-A-Stochastic-Policy-Gradient-Theorem"><a href="#I-A-Stochastic-Policy-Gradient-Theorem" class="headerlink" title="I.A. Stochastic Policy Gradient Theorem"></a>I.A. Stochastic Policy Gradient Theorem</h3><p>使用梯度上升最大化 $J(\pi_\theta)$ 需要求解梯度，根据随机策略梯度定理：</p>
<script type="math/tex; mode=display">
\nabla_\theta J(\pi_\theta)=\mathbb{E}_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta\pi_\theta(a|s)Q^\pi(s,a)]\tag{1}</script><h3 id="I-B-Stochastic-Actor-Critic"><a href="#I-B-Stochastic-Actor-Critic" class="headerlink" title="I.B. Stochastic Actor-Critic"></a>I.B. Stochastic Actor-Critic</h3><p>Actor: 由 $(1)$ 式梯度上升更新 $\theta$.</p>
<p>Critic: 参数 $w$ 通过temporal-difference learning近似 $Q^w(s,a)\approx Q^\pi(s,a)$​.</p>
<p>会产生bias，除非满足：</p>
<ol>
<li>$Q^w(s,a)=\nabla_\theta\log\pi_\theta(a|s)^\top w$，以此来满足 $(1)$ 式；</li>
<li>$w$ 最小化MSE $\epsilon^2(w)=\mathbb{E}_{s\sim\rho^\pi,a\sim\pi_\theta}\left[(Q^w(s,a)-Q^\pi(s,a))^2\right]$.</li>
</ol>
<p><strong>对1.的证明(Sutton, 1999)：</strong></p>
<p>如果达到局部最优，则有</p>
<script type="math/tex; mode=display">
\int_{\mathcal S}\rho^\pi(s)\int_{\mathcal A}\pi_\theta(s,a)\left[Q^\pi(s,a)-Q^w(s,a)\right]\nabla_wQ^w(s,a){\rm d}a{\rm d}s=0</script><p>此时如果满足</p>
<script type="math/tex; mode=display">
Q^w(s,a)=\nabla_\theta\log\pi_\theta(a|s)^\top w</script><p>则有</p>
<script type="math/tex; mode=display">
\int_{\mathcal S}\rho^\pi(s)\int_{\mathcal A}\pi_\theta(s,a)\left[Q^\pi(s,a)-Q^w(s,a)\right]\nabla_\theta\log\pi_\theta(a|s){\rm d}a{\rm d}s=0</script><p>可知估计误差 $\left[Q^\pi(s,a)-Q^w(s,a)\right]$ 与策略梯度 $\nabla_\theta\pi_\theta(a|s)$ 正交。</p>
<p>由随机策略梯度定理：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_\theta J(\pi_\theta)&=\int_{\mathcal S}\rho^\pi(s)\int_{\mathcal A}{\color{red}Q^\pi(s,a)}\nabla_\theta\pi_\theta(a|s){\rm d}a{\rm d}s\\ 
&-\int_{\mathcal S}\rho^\pi(s)\int_{\mathcal A}\pi_\theta(s,a)\left[{\color{red}Q^\pi(s,a)}-Q^w(s,a)\right]\nabla_\theta\log\pi_\theta(a|s){\rm d}a{\rm d}s\\ 
&=\int_{\mathcal S}\rho^\pi(s)\int_{\mathcal A}Q^w(s,a)\nabla_\theta\pi_\theta(a|s){\rm d}a{\rm d}s
\end{aligned}</script><p>更直观地说，估计函数 $Q^w(s,a)$ 与策略梯度在features上呈线性关系。</p>
<p>对于条件2，实际应用中可能不会去满足，而是使用temporal-difference learning来提高效率。</p>
<h3 id="I-C-Off-Policy-Actor-Critic"><a href="#I-C-Off-Policy-Actor-Critic" class="headerlink" title="I.C. Off-Policy Actor-Critic"></a>I.C. Off-Policy Actor-Critic</h3><p>Off-policy: $\beta(a|s)\ne\pi_\theta(a|s)$.</p>
<p>Performance objective一般被更改为目标策略 $\pi$ 在行为策略 $\beta$ 的状态分布上的平均价值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J_\beta(\pi_\theta)&=\int_{\mathcal S}\rho^\beta(s)V^\pi(s){\rm d}s\\ 
&=\int_{\mathcal S}\int_{\mathcal A}\rho^\beta(s)\pi_\theta(a|s)Q^\pi(s,a){\rm d}a{\rm d}s
\end{aligned}</script><p>再求梯度：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_\theta J_\beta(\pi_\theta)&\approx\int_{\mathcal S}\int_{\mathcal A}\rho^\beta(s)\nabla_\theta\pi_\theta(a|s)Q^\pi(s,a){\rm d}a{\rm d}s\\ 
&=\mathbb{E}_{s\sim\rho^\beta}\left[\int_{\mathcal A}\beta_\theta(a|s)\dfrac{\pi_\theta(a|s)}{\beta_\theta(a|s)}\nabla_\theta\log\pi_\theta(a|s)Q^\pi(s,a){\rm d}a\right]\\ 
&=\mathbb{E}_{s\sim\rho^\beta,a\sim\beta}\left[\dfrac{\pi_\theta(a|s)}{\beta_\theta(a|s)}\nabla_\theta\log\pi_\theta(a|s)Q^\pi(s,a)\right]
\end{aligned}</script><p>丢掉了 $\nabla_\theta Q^\pi(s,a)$（因为难以估计），但是可以保留local optima.</p>
<p>为什么？证明如下（来自Degris et al., Off-Policy Actor-Critic, 2012）：</p>
<p>定理：对于任何策略参数 $\theta$，令 </p>
<script type="math/tex; mode=display">
\theta'=\theta+\alpha\int_{\mathcal S}\int_{\mathcal A}\rho^\beta(s)\nabla_\theta\pi_\theta(a|s)Q^\pi(s,a){\rm d}a{\rm d}s</script><p>则存在 $\epsilon&gt;0$ 使得 $\forall\alpha&lt;\epsilon$，</p>
<script type="math/tex; mode=display">
J_\beta(\pi_{\theta'})\ge J_\beta(\pi_\theta)</script><p>具体证明过程略。由此可知这种梯度上升方式是可行的。</p>
<p>在Off-Policy Actor-Critic架构中，Critic的目标进而改为估计 $V^v(s)\approx V^\pi(s)$​.</p>
<p>由于 $Q^\pi(s,a)$ 未知，使用temporal-difference error $\delta_t=r_{t+1}+\gamma V^v(s_{t+1})-V^v(s_t)$，可证明是真实梯度的近似。</p>
<p>Actor和Critic都使用importance sampling ratio $\dfrac{\pi_\theta(a|s)}{\beta_\theta(a|s)}$ 来调整使得actions是根据 $\pi$ 来选择而不是 $\beta$.</p>
<h3 id="I-D-Gradients-of-Deterministic-Policies"><a href="#I-D-Gradients-of-Deterministic-Policies" class="headerlink" title="I.D. Gradients of Deterministic Policies"></a>I.D. Gradients of Deterministic Policies</h3><p>确定性策略用 $\mu$ 表示。</p>
<p>如果直接令 $\mu^{k+1}(s)=\arg\max\limits_aQ^{\mu^k}(s,a)$，在连续动作空间里每一步都需要求最大值，不方便。</p>
<p>替代方案是使策略以 $\nabla Q$ 的方向移动，即</p>
<script type="math/tex; mode=display">
\theta^{k+1}=\theta^k+\alpha\mathbb{E}_{s\sim\rho^{\mu^k}}\left[\nabla_\theta Q^{\mu^k}(s,\mu_\theta(s))\right]</script><p>可以看到对于策略的提升可被分解为action value对action的梯度以及策略对策略参数的梯度：</p>
<script type="math/tex; mode=display">
\theta^{k+1}=\theta^k+\alpha\mathbb{E}_{s\sim\rho^{\mu^k},a=\mu_\theta(s)}\left[\nabla_\theta\mu_\theta(s)\nabla_aQ^{\mu^k}(s,a)\right]</script><p>DPG定理证明上面的更新就是performance objective的梯度方向，即</p>
<script type="math/tex; mode=display">
\nabla_\theta J(\mu_\theta)=\mathbb{E}_{s\sim\rho^{\mu^k},a=\mu_\theta(s)}\left[\nabla_\theta\mu_\theta(s)\nabla_aQ^{\mu^k}(s,a)\right]</script><p>另外文章还证明了DP是SP的极限形式。</p>
<h4 id="I-D-1-On-Policy-Deterministic-Actor-Critic"><a href="#I-D-1-On-Policy-Deterministic-Actor-Critic" class="headerlink" title="I.D.1. On-Policy Deterministic Actor-Critic"></a>I.D.1. On-Policy Deterministic Actor-Critic</h4><p>虽然是deterministic，但是在噪声较大的情况下也能保证exploration.</p>
<p>算法流程：</p>
<script type="math/tex; mode=display">
\delta_t=r_t+\gamma Q^w(s_{t+1},a_{t+1})-Q^w(s_t,a_t)\\
w_{t+1}=w_t+\alpha_w\delta_t\nabla_wQ^w(s_t,a_t)\\
\theta_{t+1}=\theta_t+\alpha_\theta\nabla_\theta\mu_\theta(s_t)\nabla_aQ^w(s_t,a_t)|_{a=\mu_\theta(s)}</script><h4 id="I-D-2-Off-Policy-Deterministic-Actor-Critic"><a href="#I-D-2-Off-Policy-Deterministic-Actor-Critic" class="headerlink" title="I.D.2. Off-Policy Deterministic Actor-Critic"></a>I.D.2. Off-Policy Deterministic Actor-Critic</h4><p>跟stochastic情况相似，有</p>
<script type="math/tex; mode=display">
\nabla_\theta J_\beta(\mu_\theta)\approx\mathbb{E}_{s\sim\rho^\beta,a=\mu_\theta(s)}\left[\nabla_\theta\mu_\theta(s)\nabla_aQ^\mu(s,a)\right]</script><p>算法流程：</p>
<script type="math/tex; mode=display">
\delta_t=r_t+\gamma Q^w(s_{t+1},\mu_\theta(s_{t+1}))-Q^w(s_t,a_t)\\
w_{t+1}=w_t+\alpha_w\delta_t\nabla_wQ^w(s_t,a_t)\\
\theta_{t+1}=\theta_t+\alpha_\theta\nabla_\theta\mu_\theta(s_t)\nabla_aQ^w(s_t,a_t)|_{a=\mu_\theta(s)}</script><h4 id="I-D-3-Compatible-Function-Approximation"><a href="#I-D-3-Compatible-Function-Approximation" class="headerlink" title="I.D.3. Compatible Function Approximation"></a>I.D.3. Compatible Function Approximation</h4><p>相似地，$Q^w(s,a)$ 的选择需要满足下列两个条件：</p>
<ol>
<li>$\nabla_aQ^w(s,a)|_{a=\mu_\theta(s)}=\nabla_\theta\mu_\theta(s)^\top w$;</li>
<li>$w$ 最小化 $MSE(\theta,w)=\mathbb{E}[\epsilon(s;\theta,w)^\top\epsilon(s;\theta,w)]$，其中 $\epsilon(s;\theta,w)=\nabla_aQ^w(s,a)|_{a=\mu_\theta(s)}-\nabla_aQ^\mu(s,a)|_{a=\mu_\theta(s)}$.</li>
</ol>
<p>对条件1的证明类似于stochastic的情况。</p>
<p>对条件2的证明：</p>
<p>达到最小值时，梯度为0，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_wMSE(\theta,w)&=0\\ 
\mathbb{E}\left[\nabla_\theta\mu_\theta(s)\nabla_aQ^w(s,a)|_{a=\mu_\theta(s)}\right]&=\mathbb{E}\left[\nabla_\theta\mu_\theta(s)\nabla_aQ^\mu(s,a)|_{a=\mu_\theta(s)}\right]\\ 
&=\nabla_\theta J_{(\beta)}(\mu_\theta)
\end{aligned}</script><p>对于任意确定性策略 $\mu_\theta(s)$，总存在approximator，形式为</p>
<script type="math/tex; mode=display">
Q^w(s,a)=(a-\mu_\theta(s))^\top\nabla_\theta\mu_\theta(s)^\top w+V^v(s)</script><p>其中，$V^v(s)$ 可以是任何可导且独立于 $a$ 的基准函数，比如 $V^v(s)=v^\top\phi(s)$，$\phi(s)$ 为state features.</p>
<p>可以大致理解为</p>
<script type="math/tex; mode=display">
s\text{下}a\text{的动作价值}=\underbrace{\text{选}a\text{而不是}\mu_\theta(s)\text{的优势}}\limits_{A^w(s,a)=\phi(s,a)^\top w=(a-\mu_\theta(s))^\top\nabla_\theta\mu_\theta(s)^\top w}+s\text{的状态价值}</script><p>这里作者都选择用线性approximator：</p>
<blockquote>
<p>We note that a linear function approximator is not very useful for predicting action-values globally, since the action value diverges to <em>±∞</em> for large actions. However, it can still be highly effective as a <em>local</em> critic.</p>
</blockquote>
<p>总结下来：</p>
<ul>
<li>Critic 是一个线性approximator来估计action value</li>
<li>Actor向Critic给出的action value梯度方向更新参数</li>
</ul>
<h2 id="II-DQN"><a href="#II-DQN" class="headerlink" title="II. DQN"></a>II. DQN</h2><p>用神经网络（Q-network）拟合action-value.</p>
<p>DQN最初提出时仍然是stochastic的情况。</p>
<p>Q-network在每个iteration $i$ 的训练目标是最小化</p>
<script type="math/tex; mode=display">
L_i(\theta_i)=\mathbb{E}_{s,a\sim\rho(\cdot)}\left[\left(y_i-Q(s,a;\theta_i)\right)^2\right]</script><p>其中</p>
<script type="math/tex; mode=display">
y_i=\mathbb{E}_{s\sim\mathcal S}\left[r+\gamma\max\limits_{a'}Q(s',a';\theta_{i-1})|s,a\right]</script><p>即iteration $i$ 的target.</p>
<p>需要注意到一点是训练目标包含上一个迭代的参数。</p>
<p>求导可得：</p>
<script type="math/tex; mode=display">
\nabla_{\theta_i}L_i(\theta_i)=\mathbb{E}_{s,a\sim\rho(\cdot);s'\in\mathcal S}\left[r+\gamma\max\limits_{a'}Q(s',a';\theta_{i-1})-Q(s,a;\theta_i)\nabla_{\theta_i}Q(s,a;\theta_i)\right]</script><p>实际应用中不会去计算这个期望，而是每个时间步都根据单样本来更新。</p>
<p>DQN的创新点Experience Replay：可以避免strong correlation.</p>
<blockquote>
<p>By using experience replay the behavior distribution is averaged over many of its previous states, smoothing out learning and avoiding oscillations or divergence in the parameters.</p>
</blockquote>
<h2 id="III-DDPG"><a href="#III-DDPG" class="headerlink" title="III. DDPG"></a>III. DDPG</h2><p>DDPG结合了DPG和DQN.</p>
<p>DQN的问题在于连续动作空间内 $\max\limits_{a’}Q(s’,a’;\theta_{i-1})$ 不易求得，所以把DQN放进deterministic的框架中。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[DDPG] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a></p>
<p>[DPG] <a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v32/silver14.pdf">https://proceedings.mlr.press/v32/silver14.pdf</a></p>
<p>[Off-Policy AC] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1205.4839.pdf">https://arxiv.org/pdf/1205.4839.pdf</a></p>
<p>[Func Approx] <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf</a></p>
<p>[DQN] <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf</a></p>

      
    </div>
    <footer class="article-footer">
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>Charley Xiao</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/2024/04/21/dpg_ddpg/" target="_blank" title="从DPG到DDPG">http://charleyhut.github.io/2024/04/21/dpg_ddpg/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
      
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ReinforcementLearning/" rel="tag">ReinforcementLearning</a></li></ul>

      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/03/19/pgt/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Proving Policy Gradient Theorem</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#I-DPG-Deterministic-Policy-Gradient"><span class="nav-number">1.</span> <span class="nav-text">I. DPG (Deterministic Policy Gradient)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#I-A-Stochastic-Policy-Gradient-Theorem"><span class="nav-number">1.1.</span> <span class="nav-text">I.A. Stochastic Policy Gradient Theorem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-B-Stochastic-Actor-Critic"><span class="nav-number">1.2.</span> <span class="nav-text">I.B. Stochastic Actor-Critic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-C-Off-Policy-Actor-Critic"><span class="nav-number">1.3.</span> <span class="nav-text">I.C. Off-Policy Actor-Critic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-D-Gradients-of-Deterministic-Policies"><span class="nav-number">1.4.</span> <span class="nav-text">I.D. Gradients of Deterministic Policies</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-D-1-On-Policy-Deterministic-Actor-Critic"><span class="nav-number">1.4.1.</span> <span class="nav-text">I.D.1. On-Policy Deterministic Actor-Critic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-D-2-Off-Policy-Deterministic-Actor-Critic"><span class="nav-number">1.4.2.</span> <span class="nav-text">I.D.2. Off-Policy Deterministic Actor-Critic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-D-3-Compatible-Function-Approximation"><span class="nav-number">1.4.3.</span> <span class="nav-text">I.D.3. Compatible Function Approximation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#II-DQN"><span class="nav-number">2.</span> <span class="nav-text">II. DQN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#III-DDPG"><span class="nav-number">3.</span> <span class="nav-text">III. DDPG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2023 - 2024 Charley&#39;s Hut All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/scripts.js"></script>





  
<script src="/js/dialog.js"></script>









	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            Charley&#39;s Hut
          </div>
          <div class="panel-body">
            Copyright © 2024 Charley Xiao All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>
  
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
</body>
</html>